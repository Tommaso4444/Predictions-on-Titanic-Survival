{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "7804d5c8-df74-4767-86c3-fa4e98633273",
    "_uuid": "7448a7c0-7734-4912-b20f-1291c65486e4"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1e71a69f-65ea-4863-8328-7fde593da351",
    "_uuid": "9ade1083-dbb1-458e-99ec-41bb1452432c"
   },
   "source": [
    "Qui vengono importate le librerie necessarie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "493ecb5f-0a37-4ef3-9860-535cc4adb22a",
    "_uuid": "83448b18-0cf5-4c4a-af02-f1975457bbc3"
   },
   "outputs": [],
   "source": [
    "# statistical results\n",
    "from statsmodels.formula.api  import ols\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "#%matplotlib inline\n",
    "\n",
    "# machine learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import tree \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import learning_curve, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# outliers detection\n",
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo scopo di questo esercizio consiste nel combinare e mediare le previsioni sul dataset di test, partendo da comuni modelli di classificazione e regressione addestrati sul train dataset. Gli algoritmi utilizzati sono: il Support Vector Machine, il Gaussian Naive Bayes, il Decision Tree, la Random Forest, il K-Nearest Neighbors, e la Regressione Logistica. \n",
    "\n",
    "Per realizzare un classificatore Ensemble in grado di produrre un vettore di previsioni per il dataset di test, è necessario stimare vari modelli appartenenti alle famiglie selezionate, e selezionare quei modelli che presentano una migliore accuratezza in fase di convalida. Per fare questo è stata condotta un'ottimizzazione dei parametri più influenti. è stato anche selezionato il migliore modello per ogni tipo di classificatore\n",
    "\n",
    "Infine, verrà sottoposto a Kaggle un vettore di previsioni (418\\ \\ast\\ 1), i cui risultati saranno i valori medi di ogni test datapoint ottenuti considerando tutti i classificatori selezionati, e attribuendo un peso maggiore ai classificatori con la migliore accuracy in fase di convalida. Si utilizza infatti un approccio ensemble perché si tiene conto di vari classificatori."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "8cdec153-366a-4b02-b547-5a7ffc9e511e",
    "_uuid": "733663ab-99ae-4056-84bb-38d8e0206a55"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File Barbiero Tommaso Progetto SuperGuida Machine Learning e AI/train.csv does not exist: 'Barbiero Tommaso Progetto SuperGuida Machine Learning e AI/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-811d617151bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#test_df = pd.read_csv('/kaggle/input/titanic/test.csv')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#train_df = pd.read_csv('C:/Users/Tommaso/Documents/Python Scripts\\MACHINE LEARNING with UDACITY/Less2 p19 classification cars/titanic/train.csv')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtrain_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Barbiero Tommaso Progetto SuperGuida Machine Learning e AI/train.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m#test_df = pd.read_csv('C:/Users/Tommaso/Documents/Python Scripts\\MACHINE LEARNING with UDACITY/Less2 p19 classification cars/titanic/test.csv')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Barbiero Tommaso Progetto SuperGuida Machine Learning e AI/test.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python38\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python38\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python38\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python38\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python38\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1891\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File Barbiero Tommaso Progetto SuperGuida Machine Learning e AI/train.csv does not exist: 'Barbiero Tommaso Progetto SuperGuida Machine Learning e AI/train.csv'"
     ]
    }
   ],
   "source": [
    "#train_df = pd.read_csv('/kaggle/input/titanic/train.csv')\n",
    "#test_df = pd.read_csv('/kaggle/input/titanic/test.csv')\n",
    "#train_df = pd.read_csv('C:/Users/Tommaso/Documents/Python Scripts\\MACHINE LEARNING with UDACITY/Less2 p19 classification cars/titanic/train.csv')\n",
    "train_df = pd.read_csv('Barbiero Tommaso Progetto SuperGuida Machine Learning e AI/train.csv')\n",
    "#test_df = pd.read_csv('C:/Users/Tommaso/Documents/Python Scripts\\MACHINE LEARNING with UDACITY/Less2 p19 classification cars/titanic/test.csv')\n",
    "test_df = pd.read_csv('Barbiero Tommaso Progetto SuperGuida Machine Learning e AI/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c2d6cc96-539c-48aa-8fb6-c7fb4b639928",
    "_uuid": "f18a895d-ae8d-412e-abf6-03768dca605f"
   },
   "source": [
    "Combino i set di dati di train e di test in una sola, in modo da poter gestire contemporaneamente le features di entrambi i set di dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c605cf53-0f6d-4283-9af9-34c41f617dd9",
    "_uuid": "bf3b3ff3-8f61-4f65-ade7-1ff7bf16950c"
   },
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a0b366b4-9dc3-4cec-b660-89ae5cb4fa44",
    "_uuid": "a2ea7c48-91cd-432c-a62e-a24cef2c5c98"
   },
   "outputs": [],
   "source": [
    "#distance Ny Southampton 5950\n",
    "#distance Ny Cherbourg 5800\n",
    "#distance Ny Cogh 5210\n",
    "\n",
    "test_df[\"Survived\"]=np.empty(test_df.shape[0])\n",
    "test_df[\"Survived\"] = np.nan\n",
    "\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b67d5c98-401e-428a-908d-57829efcfec2",
    "_uuid": "2d619b48-95da-4d16-b6a0-95fb2856af0f"
   },
   "source": [
    "La riga seguente rende omogeneo l'ordine dei dataset di train e di test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6c0aac66-4ab8-40ca-a3e6-1d8c2a97bf81",
    "_uuid": "ea90f6fd-fd3b-415a-b466-4e2efb9c77dd"
   },
   "outputs": [],
   "source": [
    "train_df=train_df[[\"PassengerId\",\"Pclass\",\"Name\",\"Sex\",\"Age\",\"SibSp\",\"Parch\",\"Ticket\",\"Fare\",\"Cabin\",\"Embarked\",\"Survived\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "df9aabb4-7b9b-4bb1-8294-65cff6d355b7",
    "_uuid": "2dc985a5-792e-4250-90f8-58e1db7c7c17"
   },
   "source": [
    "Combino i set di dati di train e di test in una sola, in modo da poter gestire contemporaneamente le features di entrambi i set di dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "48d49f04-509d-4564-90c7-2a1e9f2fb6cc",
    "_uuid": "d0b95255-5819-41f1-ad99-c042038df9db"
   },
   "outputs": [],
   "source": [
    "all_df=[train_df,test_df]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "85b0b6fb-59ea-4da7-8cbf-a637d4e295e2",
    "_uuid": "d0c09878-af2a-4407-b34d-981ee9fb70f0"
   },
   "source": [
    "#IT: Pulizia del codice:\n",
    "    -con la riga dataset['Age'][np.isnan(dataset['Age'])] = age_avg si assegna il valore medio di età ai datapoints aventi valore \"nan\"\n",
    "    -si rende quantitativa la variabile 'Embarked' trasformandola nella distanza da New York (destinazione designata del Titanic)(Southhampton-NY: 5910 km, Cherbourg-NY: 5800 km, Queenstown: 5210 km). Si presume che il prezzo pagato dipenda anche dal porto di partenza. Quindi viene creata la variabile quantitativa 'Distance to NY'\n",
    "    - Ai valori \"0\" o \"nan\" di \"Fare\" il prezzo del biglietto viene assegnato il valore medio del prezzo del biglietto con cui la persona ha viaggiato\n",
    "    - Si crea una nuova variabile \"Familiars\" che somma  le variabili 'SibSp' (relativa a Coniugi e Cognati) e 'Parch' (relativa al numero di genitori e figli)\n",
    "    - si sposta la colonna 'Survived' per avere un'idea più distinta sulle features e la label utilizzata \n",
    "    - due datapoints del train_set (indici 61 e 829) hanno un nan value sulla colonna 'Embarked' (e conseguentemente lo avrebbero sulla colonna \"Distance to NY\". Sono stati imputati questi valori a \"Southhampton\" sia perché è il porto da cui sono partiti la maggioranza dei viaggiatori, sia perché diversi viaggiatori che hanno pagato una tariffa molto simile ai 80 pounds di queste due persone provenivano da Southhampton. D'altronde i viaggiatori saliti a Queenstown che sono più vicini agli 80 pounds sono per difetto 29.125 sterline e per eccesso 90 sterline. Quindi è poco probabile che questi viaggiatori fossero saliti dal porto Irlandese (nettamente più vicini a New York di Southhampton e Cherbourg, questi ultimi due porti sono tra loro vicini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "83a10e32-1172-4873-a007-064df6030f93",
    "_uuid": "19ff0a4c-c299-44af-8807-cc7bafe0a7e1"
   },
   "outputs": [],
   "source": [
    "# Dato che all_df è una lista composta da due datasets (train e test), tutte le operazioni di pre-processamento si riferiscono a entrambi\n",
    "for dataset in all_df:\n",
    "    \n",
    "    age_avg = dataset['Age'].mean()\n",
    "    dataset['Fare']=dataset['Fare'].replace(0,-1)\n",
    "    #assegno -1 ai valori nulli della colonna dataset['Fare']\n",
    "    dataset['Fare'][np.isnan(dataset['Fare'])] = -1\n",
    "    class_fare_avg=dataset['Fare'].groupby(dataset['Pclass']).mean()\n",
    "    first_class_fare_avg=pd.Series.tolist(class_fare_avg)[0]\n",
    "    second_class_fare_avg=pd.Series.tolist(class_fare_avg)[1]\n",
    "    third_class_fare_avg=pd.Series.tolist(class_fare_avg)[2]\n",
    "    #assegno ai valori mancanti di della colonna dataset['Fare'], le medie del prezzo pagato dalla classe di viaggio di appartenenza\n",
    "    dataset.loc[(dataset['Pclass'] ==1)&(dataset['Fare']==-1), 'Fare'] = first_class_fare_avg\n",
    "    dataset.loc[(dataset['Pclass'] ==2)&(dataset['Fare']==-1) , 'Fare'] = second_class_fare_avg\n",
    "    dataset.loc[(dataset['Pclass'] ==3)&(dataset['Fare']==-1) , 'Fare'] = third_class_fare_avg\n",
    "    #dataset['Log Fare']=np.log(dataset['Fare']+1) #evito numeri negativi nel dataset anche in caso di normalizzazione\n",
    "    #Assegno \"Southhampton\" come porto di partenza dataset['Embarked'] dei passeggeri 61 e 829 del train_set \n",
    "    dataset['Embarked']=dataset['Embarked'].fillna('S')\n",
    "    #Trasformo le informazioni sul sesso dei passeggeri in numeri: assegno 0 alle femmine, e 1 ai maschi\n",
    "    dataset['Sex'] =dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n",
    "    #Trasformo in una variabile numerica le informazioni relative al porto di partenza, in base alla distanza da New York\n",
    "    dataset[\"Distance to NY\"]=dataset['Embarked'].map({'S':5950,'C':5800,'Q':5210})\n",
    "    #Dal momento che minore è la distanza da New York, minore è stato il prezzo in media pagato dai passeggeri, \n",
    "    #divido la 'Fare' per la \"Distance to NY\", per rendere nullo l'effetto distanza\n",
    "    dataset[\"Fare per 1000km\"]= 1000*dataset['Fare']/dataset[\"Distance to NY\"]\n",
    "    # I \"Familiars\" sono la somma dei coniugi, dei cognati, dei genitori, e dei figli\n",
    "    dataset['Familiars']=dataset['SibSp']+dataset['Parch']\n",
    "    # Creo una nuova variabile, identificando le donne o i bambini. Infatti è noto che hanno avuto la precedenza di imbarco nelle scialuppe\n",
    "    dataset[\"WomenOrChildren\"]=np.zeros(dataset.shape[0])\n",
    "    #Assegno \"1\" alle e donne o i bambini, intesi come persone aventi età inferiore ai 18 anni. Assegno \"0\" ai datapoints restanti, uomini maggiorenni\n",
    "    dataset.loc[(dataset['Age'] <18) | (dataset['Sex']==0), 'WomenOrChildren'] = 1\n",
    "    # Assegno l'età media del dataset, alle persone di cui non si conosce l'età\n",
    "    dataset['Age'][np.isnan(dataset['Age'])] = age_avg\n",
    "    dataset[\"WomenOrChildren\"]=np.zeros(dataset.shape[0])\n",
    "    dataset.loc[(dataset['Age'] <18) | (dataset['Sex']==0), 'WomenOrChildren'] = 1\n",
    "    #converto la colonna dataset['WomenOrChildren'] nel tipo int invece che float\n",
    "    dataset['WomenOrChildren']=dataset['WomenOrChildren'].astype(int)\n",
    "    #sposto la colonna dataset['Survived'] in modo da poter raggruppare più facilmente le features\n",
    "    col_at_end = dataset['Survived']\n",
    "    dataset.pop('Survived')\n",
    "    dataset['Survived']=col_at_end\n",
    "    \n",
    "#mostro ad esempio il train_df risultato\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "255164f9-7ac5-4bc0-b6b3-aa361efd5d2d",
    "_uuid": "49c994cb-4d7d-4001-9b14-cc103379deee"
   },
   "source": [
    "Tolgo le colonne che non forniscono, a mio avviso, ulteriori infommazioni trasformabili in numero. Ad esempio il codice della cabina è disponibile solo per la prima classe, ed eventuali diversità tra le cabine di questa classe si riflettono nel prezzo, e quindi probabilmente sulle possibilità di sopravvivenza. La colonna 'Embarked' è già stata convertita in numero con la colonna \"Distance to NY\".\n",
    "Creo inoltre un nuovo dataset all_df che combina il train_df e il test_df, ai fini della normalizzazione delle features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "00959b59-e4a8-4224-9e24-5abef8127b57",
    "_uuid": "4f30179d-d2c2-437b-995e-40a1923759e6"
   },
   "outputs": [],
   "source": [
    "drop_elements = ['Name', 'Ticket', 'Cabin', 'SibSp','Embarked']\n",
    "train_df = train_df.drop(drop_elements, axis = 1)\n",
    "test_df=test_df.drop(drop_elements, axis = 1)\n",
    "all_df=pd.concat([train_df,test_df])\n",
    "train_df\n",
    "test_df\n",
    "all_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "25116909-6c7a-4595-86c6-07002e9f4336",
    "_uuid": "77e06667-f6cb-476b-b0d5-2e6d5a8e4e86"
   },
   "source": [
    "Ora il dataset di train ha tutte le celle piene e tutte convertite in formato numerico. Lo stesso vale per il dataset di test, eccezione fatta ovviamente per la colonna dataset['Survived'] di cui non abbiamo informazioni, e di cui si proverà a prevederne i valori mediante opportuni classificatori e tecniche di regressione"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ffeb39dc-8fce-47c7-a5a5-65d9166afa9b",
    "_uuid": "6aa6a711-ab51-4248-aef9-2e61e0440753"
   },
   "source": [
    "Con la cella del notebook seguente si mostra come inserendo come previsione di sopravvivenza l'informazione se una persona è Donna o Bambino, si riesce ad ottenere tramite Kaggle una già discreta test accuracy (circa 73%). Si utilizzeranno vari classificatori per aumentare la test accuracy, ma intanto la variabile test_df['WomenOrChildren'] appare valida nell'influenzare le chances di sopravvivenza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5d9e7a02-394a-4491-981b-91036630f2d0",
    "_uuid": "520b9051-0a01-4035-9c07-607425de3f46"
   },
   "outputs": [],
   "source": [
    "WoC=pd.Series(test_df['WomenOrChildren'])\n",
    "\n",
    "results=pd.DataFrame({'PassengerId':test_df['PassengerId'], 'Survived':WoC})\n",
    "results.to_csv(r\"WomenOrChildren.csv\", index=False) #always add r else zeros in csv are not visible\n",
    "#test accuracy submitted with Kaggle: 0.73205\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f77fb766-4663-450a-bbe6-cf616185cbb5",
    "_uuid": "35d505fc-4037-4155-98a0-238bce774704"
   },
   "source": [
    "Nella cella sottostante si elencano in una lista tutte le variabili utilizzate\n",
    "Poi chiamo X (per ora) il DataFrame di train di tutte le features utilizzate\n",
    "Chiamo y la Pandas.Series delle labels, ossia \"0\" se il passeggero è morto nel naufragio, e \"1\" se è sopravvissuto\n",
    "X_te è il dataframe delle features per quanto riguarda i datapoints di test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a5cfcc17-dca5-418d-9a22-aa14c496ee66",
    "_uuid": "b842fc31-7011-4bff-babf-a1e770303c5b"
   },
   "outputs": [],
   "source": [
    "all_variables=['Pclass','Sex','Age','Parch','Fare','Distance to NY','Fare per 1000km','Familiars','WomenOrChildren']\n",
    "\n",
    "X_all_df=all_df[all_variables]\n",
    "X=train_df[all_variables]\n",
    "y=train_df['Survived']\n",
    "X_te=test_df[all_variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d91660f6-45ce-46d4-9604-0b4030335265",
    "_uuid": "f711866f-ee0d-42c6-8d12-4265789f62d1"
   },
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5d1806eb-4c8d-463b-be54-6cc7a9108293",
    "_uuid": "dc9e7003-3b97-4593-b3ed-e32409ca1836"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "61c42fcd-963c-49d9-9211-20a756588583",
    "_uuid": "9334b9f1-d255-4195-8d83-148d2bd7d47f"
   },
   "source": [
    "**GRAFICI DESCRITTIVI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3ab4362a-f490-442c-befd-b8abfeb69a69",
    "_uuid": "354c9c75-7321-4f4f-a1af-5faf9fd267a2"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 2, squeeze=False, figsize=(14, 16))\n",
    "train_df.Age.plot.hist(bins=16, title='Passengers Age  '+'(n=891)', ax=axs[0, 0],rwidth=0.9) \n",
    "axs[0,0].set_xlabel(\"Age\")\n",
    "\n",
    "fare = train_df.Fare\n",
    "fare.plot.hist(bins=30, title='Fare  '+'(n=891)', ax=axs[0,1],rwidth=1)\n",
    "axs[0,1].set_xlabel(\"Fare\")\n",
    "\n",
    "col = ['red', 'green']\n",
    "train_df.groupby('Survived').agg('count')['Age'].plot.pie(title='Survived  '+'(n=891)', ax=axs[1,0],labels=[\"Dead\",\"Survived\"],colors=col,autopct='%1.1f%%',shadow=True)\n",
    "axs[1, 0].set_ylabel('')\n",
    "\n",
    "col = ['lightcoral', 'blue']\n",
    "train_df.groupby('Sex').agg('count')['Age'].plot.pie(title='Sex  '+'(n=891)', ax=axs[1, 1],labels=[\"Females\",\"Males\"],colors=col,autopct='%1.1f%%',shadow=True)\n",
    "axs[1, 1].set_ylabel('')\n",
    "\n",
    "col=[\"#0066ff\",\"#0099ff\",\"#00ccff\"]\n",
    "train_df.groupby('Pclass').agg('count')['Age'].plot.pie(title='Class  '+'(n=891)', ax=axs[2,0],labels=[\"1-st class\",\"2-nd class\", \"3-rd class\"],colors=col,autopct='%1.1f%%',shadow=True)\n",
    "axs[2, 0].set_ylabel('')\n",
    "\n",
    "col=[\"#0066ff\",\"lawngreen\"]\n",
    "train_df.groupby('WomenOrChildren').agg('count')['Age'].plot.pie(title='Women or Children  '+'(n=891)', ax=axs[2,1],labels=[\"Adult Male\",\"Women or Children\"],colors=col,autopct='%1.1f%%',shadow=True)\n",
    "axs[2, 1].set_ylabel('')\n",
    "\n",
    "plt.subplots_adjust(wspace=0.3, hspace=0.35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d7b8e162-d48a-4ba2-91a7-d42c3e5d3f76",
    "_uuid": "0629bb92-c5c1-4d26-98c0-2dbef1ff9fd0"
   },
   "source": [
    "Clono il dataset di train X in W per aggiungere in W delle colonne utili per la descrizione dei dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "904b0785-3e4c-41ec-ad51-6f97b852be55",
    "_uuid": "4c098e5c-9223-48db-a16f-220473d2205d"
   },
   "outputs": [],
   "source": [
    "W=X\n",
    "W['Survived']=y.values\n",
    "# Uomini e donne sopravvissuti\n",
    "W['Men survived']=np.zeros(W.shape[0])\n",
    "W.loc[(W['Survived'] ==1) & (W['Sex']==1), 'Men survived'] = 1\n",
    "W['Women survived']=np.zeros(W.shape[0])\n",
    "W.loc[(W['Survived'] ==1) & (W['Sex']==0), 'Women survived'] = 1\n",
    "\n",
    "total_men=np.round(sum(train_df['Sex']==1))\n",
    "total_women=np.round(sum(train_df['Sex']==0))\n",
    "men_survived=np.round(sum(W['Men survived']))\n",
    "men_dead=total_men-men_survived\n",
    "women_survived=np.round(sum(W['Women survived']))\n",
    "women_dead=total_women-women_survived"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f8b20eeb-9587-4e51-b9cd-9edc22fb335d",
    "_uuid": "e5b510fe-9131-46ed-a2c3-ce2a75796193"
   },
   "source": [
    "**Persone sopravvissute per classi di viaggio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7bb2dd08-f1c8-4592-b9c8-da236af2df62",
    "_uuid": "cf494bc9-4928-4161-ad11-16886dfb1d5c"
   },
   "outputs": [],
   "source": [
    "W['First-Class survived']=np.zeros(W.shape[0])\n",
    "W['Second-Class survived']=np.zeros(W.shape[0])\n",
    "W['Third-Class survived']=np.zeros(W.shape[0])\n",
    "W.loc[(W['Survived'] ==1) & (W['Pclass']==1), 'First-Class survived'] = 1\n",
    "W.loc[(W['Survived'] ==1) & (W['Pclass']==2), 'Second-Class survived'] = 1\n",
    "W.loc[(W['Survived'] ==1) & (W['Pclass']==3), 'Third-Class survived'] = 1\n",
    "total_1st_class=sum(train_df['Pclass']==1)\n",
    "total_2nd_class=sum(train_df['Pclass']==2)\n",
    "total_3rd_class=sum(train_df['Pclass']==3)\n",
    "\n",
    "first_class_survived=np.round(sum(W['First-Class survived']))\n",
    "first_class_dead=total_1st_class-first_class_survived\n",
    "second_class_survived=np.round(sum(W['Second-Class survived']))\n",
    "second_class_dead=total_2nd_class-second_class_survived\n",
    "third_class_survived=np.round(sum(W['Third-Class survived']))\n",
    "third_class_dead=total_3rd_class-third_class_survived\n",
    "third_class_survived"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3df3ccb4-7cee-4716-8819-6178bae67c91",
    "_uuid": "4ae8f39d-355c-4f49-8e02-23c524ff69f9"
   },
   "source": [
    "**Adulti e non adulti sopravvissuti**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5f4e9af9-1440-43c6-ad2c-b4ae6b6111a9",
    "_uuid": "a00d85e5-e16f-4495-9cd8-95959d759e68"
   },
   "outputs": [],
   "source": [
    "W['Non-adult survived']=np.zeros(W.shape[0])\n",
    "W['Adult survived']=np.zeros(W.shape[0])\n",
    "W.loc[(W['Survived'] ==1) & (W['Age'] <18), 'Non-adult survived'] = 1\n",
    "W.loc[(W['Survived'] ==1) & (W['Age']>=18), 'Adult survived'] = 1\n",
    "total_non_adult=np.round(sum(train_df['Age'] <18))\n",
    "total_adult=np.round(sum(train_df['Age']>=18))\n",
    "\n",
    "\n",
    "non_adult_survived=np.round(sum(W['Non-adult survived']))\n",
    "non_adult_dead=total_non_adult-non_adult_survived\n",
    "adult_survived=np.round(sum(W['Adult survived']))\n",
    "adult_dead=total_adult-adult_survived"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "748da5ee-c5d9-4440-b72f-dacdaadbf8eb",
    "_uuid": "84263c58-7843-47c3-b419-13fb9ab7d74d"
   },
   "source": [
    "**\"Donne e Bambini\" sopravvissuti**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "58552efc-0896-407a-87ff-a0211ee65b96",
    "_uuid": "1df154e4-d8f7-4c32-9aca-3a28a58e5a0d"
   },
   "outputs": [],
   "source": [
    "W['WomenOrChildren survived']=np.zeros(W.shape[0])\n",
    "W['Adult males survived']=np.zeros(W.shape[0])\n",
    "W.loc[(W['Survived'] ==1) & (W['WomenOrChildren'] ==1), 'WomenOrChildren survived'] = 1\n",
    "W.loc[(W['Survived'] ==1) & (W['WomenOrChildren'] ==0), 'Adult males survived'] = 1\n",
    "total_WomenOrChildren=np.round(sum(train_df['WomenOrChildren']==1))\n",
    "total_adult_males=np.round(sum(train_df['WomenOrChildren']==0))\n",
    "\n",
    "WomenOrChildren_survived=sum(W['WomenOrChildren survived']==1)\n",
    "WomenOrChildren_dead=total_WomenOrChildren-WomenOrChildren_survived\n",
    "adult_males_survived=np.round(sum(W['Adult males survived']==1))\n",
    "adult_males_dead=total_adult_males-adult_males_survived"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a2814afa-5ddb-47d6-8db5-a681e30f2349",
    "_uuid": "4a52296d-3e94-481b-81e1-a7a77995007d"
   },
   "source": [
    "**Costruzione del multiple bar chart**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d5caca25-6344-44a0-b48c-67191e090513",
    "_uuid": "490af980-b47b-41ef-a4f7-1187a7abfd4a"
   },
   "outputs": [],
   "source": [
    "#LISTA DEI VALORI DELLE COLONNE DEL MULTIPLE BAR CHART\n",
    "\n",
    "men_women_survived = [men_survived, women_survived]\n",
    "men_women_dead = [men_dead, women_dead]\n",
    "\n",
    "class_survived = [first_class_survived, second_class_survived, third_class_survived]\n",
    "class_dead = [first_class_dead, second_class_dead, third_class_dead]\n",
    "\n",
    "adultness_survived=[non_adult_survived,adult_survived]\n",
    "adultness_dead=[non_adult_dead,adult_dead]\n",
    "\n",
    "donnebambini_or_not_survived = [WomenOrChildren_survived,adult_males_survived]\n",
    "donnebambini_or_not_dead = [WomenOrChildren_dead,adult_males_dead]\n",
    "\n",
    "#FUNZIONE PER VISUALIZZARE I VALORI DELLE COLONNE DEL MULTIPLE BAR CHART SOPRA LE BARRE\n",
    "# codice riadattatato da https://matplotlib.org/gallery/lines_bars_and_markers/barchart.html#sphx-glr-gallery-lines-bars-and-markers-barchart-py\n",
    "def autolabel(rects,ax): \n",
    "    \"\"\"si aggiunge una label sopra ogni rettangolo, mostrando la sua altezza (che è il valore del rect)\"\"\"\n",
    "    rect_list=[]\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # offset verticale di 3 punti\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "###\n",
    "fig=plt.figure(figsize=(10,12))\n",
    "\n",
    "# Uomini e Donne Sopravvissute\n",
    "labels = ['Men', 'Women']\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "ax = plt.subplot2grid((2,2),(0,0),rowspan=1,colspan=1)\n",
    "rects1 = ax.bar(x - width/2, men_women_survived, width, label='Survived', color='green')\n",
    "rects2 = ax.bar(x + width/2, men_women_dead, width, label='Dead', color='red')\n",
    "ax.set_ylabel('Passengers')\n",
    "ax.set_title('Number of Men and Women survived')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "autolabel(rects1,ax)\n",
    "autolabel(rects2,ax)\n",
    "\n",
    "# Persone Sopravvissute per classi di passeggero\n",
    "labels = ['1st class', '2nd class','3rd class']\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "ax1 = plt.subplot2grid((2,2),(0,1),rowspan=1,colspan=1)\n",
    "rects1 = ax1.bar(x - width/2, class_survived, width, label='Survived', color='green')\n",
    "rects2 = ax1.bar(x + width/2, class_dead, width, label='Dead', color='red')\n",
    "ax1.set_ylabel('Passengers')\n",
    "ax1.set_title('People survived by passenger class')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(labels)\n",
    "ax1.legend()\n",
    "autolabel(rects1,ax1)\n",
    "autolabel(rects2,ax1)\n",
    "\n",
    "# Bambini e Adulti Sopravvissuti\n",
    "labels = ['Non-adult', 'Adult']\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "ax2 = plt.subplot2grid((2,2),(1,0),rowspan=1,colspan=1)\n",
    "rects1 = ax2.bar(x - width/2, adultness_survived, width, label='Survived', color='green')\n",
    "rects2 = ax2.bar(x + width/2, adultness_dead, width, label='Dead', color='red')\n",
    "ax2.set_ylabel('Passengers')\n",
    "ax2.set_title('Number of Non-adults and Adults survived')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(labels)\n",
    "ax2.legend()\n",
    "autolabel(rects1,ax2)\n",
    "autolabel(rects2,ax2)\n",
    "\n",
    "# Donne&Bambini e AdultiMaschi Sopravvissuti\n",
    "labels = ['Wom & Non-adult', 'Adult Males']\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "ax3 = plt.subplot2grid((2,2),(1,1),rowspan=1,colspan=1)\n",
    "rects1 = ax3.bar(x - width/2, donnebambini_or_not_survived, width, label='Survived', color='green')\n",
    "rects2 = ax3.bar(x + width/2, donnebambini_or_not_dead, width, label='Dead', color='red')\n",
    "ax3.set_ylabel('Passengers')\n",
    "ax3.set_title('Women or Children survived')\n",
    "ax3.set_xticks(x)\n",
    "ax3.set_xticklabels(labels)\n",
    "ax3.legend()\n",
    "autolabel(rects1,ax3)\n",
    "autolabel(rects2,ax3)\n",
    "\n",
    "plt.subplots_adjust(wspace=0.3, hspace=0.35)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4aa33009-e7ac-454f-85f9-24e50a54eabd",
    "_uuid": "262c5357-df79-44e9-b8d0-71baa0b0252e"
   },
   "outputs": [],
   "source": [
    "pass_class=train_df.groupby(\"Pclass\")\n",
    "pass_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a1f1bd66-cb23-4b42-ab17-6da0115d69df",
    "_uuid": "22d9d4b4-5f64-45aa-a3eb-335c590e5e91"
   },
   "source": [
    "**RIDUZIONE DELLA DIMENSIONALITA' ATTRAVERSO LA STEPWISE BACKWARD REGRESSION**\n",
    "\n",
    "Si è voluto verificare se i modelli forniscono migliori indici di accuracy di convalida, ricorrendo ad una riduzione del numero di caratteristiche.\n",
    "Si è voluto utilizzare l'approccio della stepwise backward regression, utile per l'eliminazione delle variabili non significative. \n",
    "In questo caso la misura della significatività è data dal p-value. Maggiore è il p-value \"P>|t|\", minore è la statistica T (che è la standardizzazione dei coefficienti di regressione) e minore è la significatività della variabile. Assumendo la normalità delle variabili, e assumendo l'ipotesi nulla che le variabili siano indipendenti dalla Y, il p-value misura la probabilità di ottenere statistiche T uguali o meno probabili di quello osservato durante il test. Dunque, quanto più  T è grande (il coefficiente di regressione è grande rispetto alla sua deviazione standard), tanto minore sarà il p-value, e tanto minore sarà la plausibilità dell'ipotesi nulla di indipendenza\n",
    "Una tipica soglia massima scelta per il p-value è 0.05, che corrisponde a una statistica T = 2 (in caso di perfetta normalità dei coefficienti di regressione corrisponde a 1.96)\n",
    "L'algoritmo utilizzato scarta di volta in volta la variabile con il p-value più alto (ad eccezione dell'intercetta), fintanto che rimangono le sole variabili che presentano un p-value inferiore a 0.05. Per fare questo, una volta eliminata una variabile viene ristimato il modello di regressione\n",
    "Le variabili rimaste sono significative, mentre il coefficiente delle variabili scartate viene considerato pari a 0. Un altro algoritmo di riduzione della dimensionalità è l'Analisi delle Componenti Principali (PCA), in cui dapprima vengono ortogonalizzate le diverse features, che quindi diventano tra loro indipendenti. Poi viene via via scelta la componente principale che è la variabile ortogonalizzata che presenta la massima varianza. Il PCA, tuttavia, non tiene conto della label, e si presta di più a casi di unsupervised learning. La selezione delle variabili basata sulla Stepwise backward regression riguarda direttamente la causalità tra le features e la variabile target, pertanto è un appropriato metodo di selezione delle variabili"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d2fdbd7f-5e67-491d-8ada-3a6a555a4b5c",
    "_uuid": "9b0e2791-d428-4bc2-8878-4f56bb34c8ee"
   },
   "outputs": [],
   "source": [
    "#STEPWISE LINEAR REGRESSION\n",
    "\n",
    "def stepwise_selection(X, y, \n",
    "                       initial_list=['Pclass', 'Sex','Age','Parch','Fare','Distance to NY','Fare per 1000km','Familiars','WomenOrChildren'], \n",
    "                       threshold_out = 0.05, \n",
    "                       verbose=True):\n",
    "        \n",
    "    included = list(initial_list)\n",
    "    while True:\n",
    "        changed=False\n",
    "        \n",
    "\n",
    "        # backward step\n",
    "        # si istanzia un modello di regressione di tipo OLS (Ordinary Least Squares)\n",
    "        model = sm.OLS(y, sm.add_constant(X[included])).fit()\n",
    "        # si stampano i risultati del modello: coefficienti beta, statistiche T, p-value, R^2, adjusted R^2 ecc. All'inizio vengono prese in considerazione tutte le variabili\n",
    "        print(model.summary()) \n",
    "        # si considerano i p-value di tutte le variabili tranne quello dell'intercetta, che comunque vogliamo mantenere\n",
    "        pvalues = model.pvalues.iloc[1:]\n",
    "        worst_pval = pvalues.max() # si identifica il massimo p-value (che è legato alla variabile meno significativa)\n",
    "        if worst_pval > threshold_out: #threshold_out=0.05\n",
    "            changed=True #il modello cambia. Quindi si deve compiere una nuova regressione\n",
    "            worst_feature = pvalues.argmax() #si identifica la feature che ha il max p-value\n",
    "            included.remove(included[worst_feature]) # si rimuove la caratteristica da quelle da includere nel modello di regressione\n",
    "            if verbose:\n",
    "                print('Drop {:30} with p-value {:.6}'.format(worst_feature, worst_pval))\n",
    "        if not changed: \n",
    "            # se tutte le variabili soddisfano la soglia \"threshold_out\" non si deve più cambiare il modello e fare una nuova regressione. \n",
    "            #Si esce quindi dal ciclo con le sole variabili significative\n",
    "            break\n",
    "    return included\n",
    "\n",
    "result_backward_stepwise_regression = stepwise_selection(X, y)\n",
    "# Si stampano le sole variabili significative\n",
    "print(\"\\n\\nVariabili significative (p-value<0.05): \",result_backward_stepwise_regression)\n",
    "\n",
    "# elenco delle variabili significative\n",
    "variables=result_backward_stepwise_regression\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CONTROLLO SULL'IMPIEGO DI TUTTE LE VARIABILI O SOLO DELLE SIGNIFICATIVE**\n",
    "\n",
    "Nel primo caso usa: \n",
    "full_variables=True\n",
    "\n",
    "Nel secondo usa:\n",
    "full_variables=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_variables=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if full_variables==True:\n",
    "    X=train_df[all_variables]   ####### ALL FEATURES X=train_df[all_variables]  ######### FEATURE SELECTION X=train_df[variables] \n",
    "else:\n",
    "    X_te=test_df[variables] ####### ALL FEATURES X_te=test_df[all_variables]  ######### FEATURE SELECTION X_te=test_df[variables]\n",
    "\n",
    "# Definizione del features_train set e del features_test set\n",
    "features_train=X.values\n",
    "features_test=X_te.values\n",
    "\n",
    "#matrice delle features in cui vengono inclusi i datapoints di tutti i due datasets (train e test). La matrice è utile per la normalizzazione delle features\n",
    "all_df_features=X_all_df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e9562323-7be6-45bd-9a7e-e0c51f3b8780",
    "_uuid": "94675a38-dca5-4852-8979-981639abf0d5"
   },
   "source": [
    "outliers detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0fbb931b-405a-4d6a-b9a9-177631ecff22",
    "_uuid": "337f5ff5-bbf0-41db-a4eb-8733b8615258"
   },
   "outputs": [],
   "source": [
    "# Rimozione outliers nei dati secondo il modello IsolationForest\n",
    "iso = IsolationForest(contamination=0.025, random_state=69)\n",
    "yhat = iso.fit_predict(train_df)\n",
    "\n",
    "train_df['Mask']=yhat  #se train_df['Mask']=1 l'esempio non è outlier, altrimenti lo è\n",
    "train_df = train_df[train_df['Mask']==1]\n",
    "\n",
    "# Definizione di features_train, SENZA GLI OUTLIERS\n",
    "#### ALL FEATURES vs SELECTED FEATURES\n",
    "if full_variables==True:\n",
    "    features_train=train_df[all_variables] \n",
    "else:\n",
    "    features_train=train_df[variables]\n",
    "    \n",
    "labels_train=train_df[\"Survived\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "704043a5-572c-43df-9231-0142549af8d2",
    "_uuid": "ab2bbcb4-de96-4a07-9475-bce4662a3e5e"
   },
   "source": [
    "**NORMALIZZAZIONE DELLE FEATURES** e rimozione outliers\n",
    "\n",
    "Normalizzazione delle features, tenendo conto sia del train e del test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2c6dcdf7-919a-4b15-a31a-8ce1b74a4245",
    "_uuid": "e0d2c8e9-a520-480e-82d5-47c207ac4cc8"
   },
   "outputs": [],
   "source": [
    "# Istanzio un nuovo oggetto MinMaxScaler capace di normalizzare le caratteristiche\n",
    "\n",
    "\n",
    "\n",
    "mms=MinMaxScaler()\n",
    "\n",
    "# Definizione delle FEATURES su cui vogliamo effettuare la NORMALIZZAZIONE\n",
    "\n",
    "#### ALL FEATURES vs SELECTED FEATURES\n",
    "if full_variables==True:\n",
    "    all_features_norm=mms.fit_transform(X_all_df)\n",
    "else:\n",
    "    all_features_norm=mms.fit_transform(X_all_df[['Pclass', 'Sex', 'Age', 'Familiars', 'WomenOrChildren']])\n",
    "\n",
    "features_train_norm=all_features_norm[:891]\n",
    "features_test_norm=all_features_norm[891:] # le colonne di features_test_norm dipendono dall'\"if\" precedente\n",
    "labels_train_norm=y.values\n",
    "\n",
    "#RIMOZIONE degli outliers sui dati NORMALIZZATI\n",
    "\n",
    "#Definizione del DATASET in base alle FEATURES SELEZIONATE\n",
    "#### ALL FEATURES vs SELECTED FEATURES\n",
    "if full_variables==True:\n",
    "    train_df_norm=pd.DataFrame(features_train_norm, columns=['Pclass','Sex','Age','Parch','Fare','Distance to NY','Fare per 1000km','Familiars','WomenOrChildren'])\n",
    "else:\n",
    "    train_df_norm=pd.DataFrame(features_train_norm, columns=['Pclass', 'Sex', 'Age', 'Familiars', 'WomenOrChildren'])\n",
    "train_df_norm[\"Survived\"]=labels_train_norm\n",
    "yhat_norm = iso.fit_predict(train_df_norm)\n",
    "train_df_norm['Mask']=yhat_norm  #se train_df['Mask']=1 l'esempio non è outlier, altrimenti lo è\n",
    "train_df_norm = train_df_norm[train_df_norm['Mask']==1]\n",
    "\n",
    "# DEFINIZIONE delle features_train_norm ossia delle variabili di ADDESTRAMENTO NORMALIZZATE\n",
    "#### ALL FEATURES vs SELECTED FEATURES\n",
    "if full_variables==True:\n",
    "    features_train_norm=train_df_norm[all_variables]\n",
    "else:\n",
    "    features_train_norm=train_df_norm[variables]\n",
    "\n",
    "     \n",
    "labels_train_norm=train_df_norm[\"Survived\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5fb509e3-af3e-45ee-b1be-c748c26460ab",
    "_uuid": "028ac212-5995-4d50-988c-8ddc5cecfd62"
   },
   "source": [
    "Per esempio mostro la normalizzazione delle features del dataset di addestramento. Per assicurare che a medesimi valori delle features corrispondano medesimi valori normalizzati è stato utilizzato il dataset all_df, sono stati calcolati i valori normalizzati, e poi è stato diviso nelle due parti relative ai dati di addestramento (fino alla riga 891) e ai dati di test (oltre la riga 891).\n",
    "Le operazioni di normalizzazione sono necessarie nei modelli di classificazione basati sulle distanze tra i datapoints. Se due features hanno campi di variazione differenti dovuti alla scala, la componente delle distanze basata sulle caratteristiche avente un range più ampio risulterà indebitamente sopravvalutata, compromettendo l'affidabilità della classificazione. Tra i modelli impiegati in questo esercizio che sono influenzati dalla distanza troviamo: il Support Vector Machine e il K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d80547de-0ad9-4929-ac0f-039cda6dda95",
    "_uuid": "055366b4-34e7-4f4e-a797-4548db7e0d88"
   },
   "source": [
    "**CREAZIONE FOLD DI CONVALIDA**\n",
    "\n",
    "Nella cella seguente uso un oggetto Kfold, utile per la convalida incrociata dei classificatori e dei modelli di regressione. In questo caso si divide il train set in 10 parti, delle quali su 9 verrà addestrato un modello, di cui verrà valutata la validità in termini di accuracy sulla parte restante. La stessa operazione verrà compiuta cambiando la parte su cui il modello viene valutato, finché tutti i set di convalida non verranno utilizzati\n",
    "Si usa shuffle=True per selezionare in modo casuale le 10 parti secondo un fisso random_state, che assicura i medesimi risultati in caso di ripetizione del codice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0b6ebfa1-ca9b-45de-b781-e64dce6e3210",
    "_uuid": "b7b6d702-38a9-4756-be8e-f4592ebbb95d"
   },
   "outputs": [],
   "source": [
    "kf=KFold(10, shuffle=True, random_state=69) \n",
    "\n",
    "#10 sono le k divisioni del dataset; shuffle = True mescolo casualmente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "64546ec7-b89b-4ac9-9018-c666fe52bf3f",
    "_uuid": "4e0435db-a066-4221-bda7-7e8f188a9cbf"
   },
   "source": [
    "**CLASSIFICATORI**\n",
    "**SUPPORT VECTOR MACHINE**\n",
    "\n",
    "L’idea principale che sta alla base di un algoritmo di Support Vector Machine è la massimizzazione del margine tra l'iperpiano di separazione e i punti appartenenti ad una certa classe (label) più vicini a questo iperpiano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ff2b17bf-f3d9-4916-b920-1cc5adaac863",
    "_uuid": "8bc0fe88-1dd2-431f-b898-141a0eef335d"
   },
   "outputs": [],
   "source": [
    "validation_accuracies=[]\n",
    "best_estimators=[]\n",
    "#Definisco un dizionario dei parametri sui quali voglio calibrare il classificatore. In questo caso i parametri costituiscono la chiave, \n",
    "#i rispettivi valori sono invece le liste che a loro volta contengono i possibili valori su cui voglio addestrare il modello.\n",
    "parameters = {'kernel':['rbf'],'C':[13500,14000,15000,16000], 'gamma': [0.075,0.08,0.085]}\n",
    "#istanzio un classificatore di tipo support vector machine\n",
    "clf=svm.SVC()\n",
    "#creo un oggetto di tipo GridSearchCV che si concatena al clf definito in precedenza. é a tutti gli effetti un classificatore che viene addestrato su tutte le possibili\n",
    "#combinazioni di parametri presenti all'interno del dizionario \"parameters\". \n",
    "#Si noti che questa operazione viene ripetuta su tutti i metodi di classificazione e regressione \n",
    "clf=GridSearchCV(clf,parameters,refit=True,cv=kf) \n",
    "#addestro l'insieme dei classificatori sulle features normalizzate in quanto altrimenti i classificatori considerano le distanze così come sono ignorando i diversi ranges delle variabili\n",
    "# Prima del fitting è dunque necessario portare le features sulla stessa scala, utilizzando le features normalizzate\n",
    "clf.fit(features_train_norm,labels_train_norm)\n",
    "# stampo i parametri del migliore modello in fase di convalida, ossia i parametri del modello che garantiscono la migliore accuratezza media sui 10 KFold.\n",
    "# questa operazione viene ripetuta su tutti i metodi di classificazione e regressione\n",
    "print(\"Miglior classificatore Support Vector Machine \",clf.best_params_)\n",
    "best_par=clf.best_params_ #recupero i parametri del migliore modello\n",
    "df_perf_SVM=pd.DataFrame()\n",
    "mean_accuracies=clf.cv_results_['mean_test_score'] #accuratezza media dei modelli di classificazione usati\n",
    "parameters_tested=clf.cv_results_['params'] #parametri usati dai singoli modelli di classificazione\n",
    "type_clf=\"svm.SVC\"\n",
    "# Voglio costruire un DataFrame per valutare le performances (df_perf_SVM['performances']) dei vari classificatori utilizzati in base a:\n",
    "#df_perf_SVM['type']: cioé il tipo di classificatore\n",
    "#df_perf_SVM['parameters']: cioé i parametri usati\n",
    "#Ottenuti i risultati dei vari classificatori di tutti i tipi si costruirà un unico DataFrame che permetterà di confrontare le performances, e poi successivamente calcolare le previsioni dei classificatori selezionati\n",
    "#Saranno selezionati i classificatori oltre una certa soglia di accuratezza in fase di convalida e i migliori di ogni tipologia\n",
    "clf_type_list=[]\n",
    "for i in range(len(parameters_tested)): #costruisco una lista lunga il numero di classificatori usati (cioé il numero di combinazioni dei parametri) che contiene altrettante stringhe uguali che indicano il tipo di classificatore\n",
    "    clf_type_list.append(type_clf)\n",
    "df_perf_SVM['type'] = clf_type_list\n",
    "df_perf_SVM['parameters']=parameters_tested #l'utilizzo di un DataFrame che contenga la lista (o meglio il dizionario) di parametri usati, permette successivamente di recuperare facilmente i parametri, \n",
    "#per poi calcolare le previsioni\n",
    "df_perf_SVM['performances']=mean_accuracies\n",
    "df_perf_SVM\n",
    "#definisco il modello di classificazione con i parametri del miglior modello, \n",
    "#così posso \"fittarlo\" e per infine calcolare il vettore di previsioni (418 * 1) della label, utilizzando le features del dataset di test (in questo caso normalizzate) \n",
    "best_clf_SVM=svm.SVC(kernel=best_par['kernel'],C=best_par['C'],gamma=best_par['gamma'])\n",
    "best_clf_SVM.fit(features_train_norm,labels_train_norm) #fit del classificatore\n",
    "#fitted_preds=best_clf_SVM.predict(features_train_norm) #previsione dentro il campione (non necessario)\n",
    "preds_SVM=best_clf_SVM.predict(features_test_norm) #PREVISIONE della label \"Survived\" basandosi sulle FEATURES del dataset di TEST (in questo caso normalizzate)\n",
    "#Costruzione di un DataFrame delle previsioni del miglior modello del tipo, e creazione di un file sottoponibile a Kaggle\n",
    "results=pd.DataFrame({'PassengerId':test_df['PassengerId'], 'Survived':preds_SVM})\n",
    "results.to_csv(r\"C:\\Users\\Tommaso\\Desktop\\Titanic_prediction_SVM.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "47c60f9d-0b64-4ddd-bf9b-6759284b51c3",
    "_uuid": "d50ae2ae-5390-4670-9a28-625e7daa82bc"
   },
   "source": [
    "Nelle successive celle vengono ottenute le performances dei vari tipi di classificatori oltre a Support Vector Machine: Gaussian Naive Bayes,Decision Tree, Random Forest, K-Nearest Neighbor,e Logistic Regression. Siccome il procedimento è analogo al codice commentato dettagliatamente per il classificatore SVM, mi concentrerò maggiormente sugli aspetti statistici"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bb1edb1f-49cd-4c92-b35d-11e5e83f206a",
    "_uuid": "212b47fb-3152-4663-8475-7c6f6c404047"
   },
   "source": [
    "#### Gaussian NB**\n",
    "Le features\"train_survived_ratio\" e \"train_deaths_ratio\" sono le probabilità a priori delle labels (respectively \"1\" (life) and \"0\" (death)), P(label=0)=0.616 e P(label=1)=0.383. I valori di questi parametri possono leggermente migliorare la validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "14b9d433-c4c9-480b-a0f6-a5c5f5140a57",
    "_uuid": "d01024a3-e769-4f15-8592-d0039b914ce7"
   },
   "outputs": [],
   "source": [
    "\n",
    "train_deaths_ratio=1-np.mean(train_df[\"Survived\"])\n",
    "train_survived_ratio=np.mean(train_df[\"Survived\"])\n",
    "parameters = {'priors':[[0.5,0.5],[train_deaths_ratio,train_survived_ratio]],'var_smoothing': [1e-9,1e-8]}\n",
    "clf=GaussianNB()\n",
    "# istanziazione di una griglia di classificatori sulla base del dizionario dei parametri (parameters)\n",
    "clf=GridSearchCV(clf,parameters,refit=True,cv=kf)\n",
    "clf.fit(features_train,labels_train)\n",
    "print(\"Miglior classificatore GaussianNB \",clf.best_params_)\n",
    "best_par=clf.best_params_\n",
    "df_perf_NB=pd.DataFrame() \n",
    "mean_accuracies=clf.cv_results_['mean_test_score']\n",
    "parameters_tested=clf.cv_results_['params']\n",
    "type_clf=\"GaussianNB\"\n",
    "clf_type_list=[]\n",
    "for i in range(len(parameters_tested)):\n",
    "    clf_type_list.append(type_clf)\n",
    "#Costruzione DataFrame delle performance di accuratezza in fase di validazione\n",
    "df_perf_NB['type'] = clf_type_list\n",
    "df_perf_NB['parameters']=parameters_tested\n",
    "df_perf_NB['performances']=mean_accuracies\n",
    "#Dichiarazione del miglior classificatore (per accuratezza in fase di validazione)\n",
    "best_clf_NB=GaussianNB(priors=best_par['priors'],var_smoothing=best_par['var_smoothing'])\n",
    "best_clf_NB.fit(features_train,labels_train)\n",
    "#Previsioni dataset di test\n",
    "preds_NB=best_clf_NB.predict(features_test)\n",
    "#Costruzione di un DataFrame delle previsioni del miglior modello del tipo, e creazione di un file sottoponibile a Kaggle\n",
    "results=pd.DataFrame({'PassengerId':test_df['PassengerId'], 'Survived':preds_NB})\n",
    "results.to_csv(r\"C:\\Users\\Tommaso\\Desktop\\Titanic_prediction_NB.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "291a1398-4e63-4e42-b89c-ac697c757a83",
    "_uuid": "f202ad0f-4ce1-449f-b3ba-19dc5bad5b8e"
   },
   "source": [
    "**DECISION TREE**\n",
    "\n",
    "Questo modello di classificazione è un sistema di suddivisione dei dati di addestramento, prendendo decisioni sulla base di una sequenza di domande. Ogni nodo (che contiene dati) viene ripartito in ulteriori nodi massimizzando l'Information Gain. Per ulteriori dettagli si consulti la relazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c86d3c91-226d-425e-9649-4fe242b4f219",
    "_uuid": "a695a88f-e533-4896-b14f-b86f883b1934"
   },
   "outputs": [],
   "source": [
    "parameters = {'max_depth':[7,8,10],'min_samples_split': [9,10,11,12],'min_samples_leaf':[1,2,3],'random_state':[69]}\n",
    "clf=tree.DecisionTreeClassifier(criterion=\"gini\")\n",
    "# istanziazione di una griglia di classificatori sulla base del dizionario dei parametri (parameters)\n",
    "clf=GridSearchCV(clf,parameters,refit=True,cv=kf)\n",
    "clf.fit(features_train,labels_train)\n",
    "print(\"Miglior classificatore Decision Tree \",clf.best_params_)\n",
    "best_par_DT=clf.best_params_\n",
    "df_perf_DT=pd.DataFrame() \n",
    "mean_accuracies=clf.cv_results_['mean_test_score']\n",
    "parameters_tested=clf.cv_results_['params']\n",
    "type_clf=\"tree.DecisionTreeClassifier\"\n",
    "clf_type_list=[]\n",
    "for i in range(len(parameters_tested)):\n",
    "    clf_type_list.append(type_clf)\n",
    "#Costruzione DataFrame delle performance di accuratezza in fase di validazione\n",
    "df_perf_DT['type'] = clf_type_list\n",
    "df_perf_DT['parameters']=parameters_tested\n",
    "df_perf_DT['performances']=mean_accuracies\n",
    "#Dichiarazione del miglior classificatore (per accuratezza in fase di validazione)\n",
    "best_clf_DT=tree.DecisionTreeClassifier(max_depth=best_par_DT['max_depth'],min_samples_split=best_par_DT['min_samples_split'],min_samples_leaf=best_par_DT['min_samples_leaf'])\n",
    "best_clf_DT.fit(features_train,labels_train)\n",
    "\n",
    "fitted_preds=best_clf_DT.predict(features_train)\n",
    "#Previsioni dataset di test\n",
    "preds_DT=best_clf_DT.predict(features_test)\n",
    "#Costruzione di un DataFrame delle previsioni del miglior modello del tipo, e creazione di un file sottoponibile a Kaggle\n",
    "results=pd.DataFrame({'PassengerId':test_df['PassengerId'], 'Survived':preds_DT})\n",
    "results.to_csv(r\"C:\\Users\\Tommaso\\Desktop\\Titanic_prediction_DT.csv\", index=False)\n",
    "\n",
    "df_perf_DT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "57f9d9df-5023-4f0f-827d-33667f03c09b",
    "_uuid": "5f08a32b-b2e5-467f-8f34-d6274e16da38"
   },
   "source": [
    "Codice utile per la rappresentazione grafica del Decision Tree ottimo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8def1e5b-7abc-4b66-ad11-af80c420b1cf",
    "_uuid": "f482f2b8-03ca-4241-abd7-b63f0fa0be06"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(50, 24))\n",
    "tree.plot_tree(best_clf_DT, fontsize=6)\n",
    "plt.savefig('tree_high_dpi', dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f3544612-b63c-4392-94c7-a0ed633fc15b",
    "_uuid": "36747b4e-f967-47b4-9133-6899969d8f74"
   },
   "source": [
    "**RANDOM FOREST with optimized trees**\n",
    "\n",
    "Il classificatore Random Forest è un esempio di ensemble method nel machine learning. Infatti, una foresta casuale può essere considerata come un insieme (ensemble) di alberi decisionali. In particolare il classificatore Random Forest, attraverso una votazione a maggioranza delle previsioni dei singoli alberi decisionali, calcola un vettore di previsioni. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "235296bf-8b9b-424b-b253-de8978102654",
    "_uuid": "7a472096-3636-4f40-a677-9ff76c685e17"
   },
   "outputs": [],
   "source": [
    "###RANDOM FOREST\n",
    "# Per gli alberi della foresta si utilizzano i parametri ottimizzati degli alberi decisionali (Decision Tree).\n",
    "#  Ho riscontrato che utilizzando i parametri ottimizzati si ottiene in fase di validazione un'accuratezza leggermente superiore (2-3% in più)\n",
    "# n_estimators\n",
    "parameters = {'n_estimators':[30,40,50,60],'max_features':[2,3,4,5],'n_jobs':[4],'max_depth':[best_par_DT['max_depth']],'min_samples_split':[best_par_DT['min_samples_split']],'min_samples_leaf':[best_par_DT['min_samples_leaf']],'random_state':[69]}\n",
    "clf=RandomForestClassifier(criterion='gini')\n",
    "# istanziazione di una griglia di classificatori sulla base del dizionario dei parametri (parameters)\n",
    "clfRF=GridSearchCV(clf,parameters,refit=True,cv=kf)\n",
    "clfRF.fit(features_train,labels_train)\n",
    "print(\"Miglior classificatore Random Forest \",clfRF.best_params_)\n",
    "best_par_RF=clfRF.best_params_\n",
    "df_perf_RF=pd.DataFrame() \n",
    "mean_accuracies=clfRF.cv_results_['mean_test_score']\n",
    "parameters_tested=clfRF.cv_results_['params']\n",
    "type_clf=\"RandomForestClassifier\"\n",
    "clf_type_list=[]\n",
    "for i in range(len(parameters_tested)):\n",
    "    clf_type_list.append(type_clf)\n",
    "#Costruzione DataFrame delle performance di accuratezza in fase di validazione\n",
    "df_perf_RF['type'] = clf_type_list\n",
    "df_perf_RF['parameters']=parameters_tested\n",
    "df_perf_RF['performances']=mean_accuracies\n",
    "#Dichiarazione del miglior classificatore (per accuratezza in fase di validazione)\n",
    "best_clf_RF=RandomForestClassifier(n_estimators=best_par_RF['n_estimators'],max_features=best_par_RF['max_features'],max_depth=best_par_DT['max_depth'],min_samples_split=best_par_DT['min_samples_split'],min_samples_leaf=best_par_DT['min_samples_leaf'])\n",
    "\n",
    "best_clf_RF.fit(features_train,labels_train)\n",
    "fitted_preds=best_clf_RF.predict(features_train)\n",
    "#Previsioni dataset di test\n",
    "preds_RF=best_clf_RF.predict(features_test)\n",
    "#Costruzione di un DataFrame delle previsioni del miglior modello del tipo, e creazione di un file sottoponibile a Kaggle\n",
    "results=pd.DataFrame({'PassengerId':test_df['PassengerId'], 'Survived':preds_RF})\n",
    "results.to_csv(r\"C:\\Users\\Tommaso\\Desktop\\Titanic_prediction_RF.csv\", index=False)\n",
    "\n",
    "df_perf_RF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2cf49154-c7db-46f8-b88f-9e0184ac5373",
    "_uuid": "2969190c-6b59-4925-8e06-d6a393ab8694"
   },
   "source": [
    "**K-NEAREST NEIGHBORS**\n",
    "\n",
    "Questo metodo di classificazione consiste nell’assegnare ad un qualsiasi punto, in uno spazio vettoriale multidimensionale, avente coordinate x_1, x_2,..., x_m, la label prevalente di un certo numero k di train datapoints vicini. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ffdef673-832d-44ae-88b6-8018d44a9c55",
    "_uuid": "f6b2d84e-adcd-4200-937a-83b00783971d"
   },
   "outputs": [],
   "source": [
    "parameters={'n_neighbors':[5,15,21,20,25,30,31,35,41],'weights': ['uniform']}\n",
    "clf=KNeighborsClassifier()\n",
    "# istanziazione di una griglia di classificatori sulla base del dizionario dei parametri (parameters)\n",
    "clf=GridSearchCV(clf,parameters,refit=True,cv=kf)\n",
    "clf.fit(features_train_norm,labels_train_norm)\n",
    "print(\"Miglior classificatore KNeighborsClassifier \",clf.best_params_)\n",
    "best_par=clf.best_params_\n",
    "df_perf_KNN=pd.DataFrame() \n",
    "mean_accuracies=clf.cv_results_['mean_test_score']\n",
    "parameters_tested=clf.cv_results_['params']\n",
    "type_clf=\"KNeighborsClassifier\"\n",
    "clf_type_list=[]\n",
    "for i in range(len(parameters_tested)):\n",
    "    clf_type_list.append(type_clf)\n",
    "#Costruzione DataFrame delle performance di accuratezza in fase di validazione\n",
    "df_perf_KNN['type'] = clf_type_list\n",
    "df_perf_KNN['parameters']=parameters_tested\n",
    "df_perf_KNN['performances']=mean_accuracies\n",
    "#Dichiarazione del miglior classificatore (per accuratezza in fase di validazione)\n",
    "best_clf=KNeighborsClassifier(n_neighbors=best_par['n_neighbors'],weights=best_par['weights'])\n",
    "best_clf.fit(features_train_norm,labels_train_norm)\n",
    "#Previsioni dataset di test\n",
    "preds_KNN=best_clf.predict(features_test_norm)\n",
    "#Costruzione di un DataFrame delle previsioni del miglior modello del tipo, e creazione di un file sottoponibile a Kaggle\n",
    "results=pd.DataFrame({'PassengerId':test_df['PassengerId'], 'Survived':preds_KNN})\n",
    "results.to_csv(r\"C:\\Users\\Tommaso\\Desktop\\Titanic_prediction_KNN.csv\", index=False)\n",
    "\n",
    "df_perf_KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "334eced5-79d5-4f0e-84db-6e03be36e5bf",
    "_uuid": "53b876b6-331d-4f34-8dc9-547fcb913727"
   },
   "source": [
    "**LOGISTIC REGRESSION.**\n",
    "Viene usata la Penalty L2\n",
    "\n",
    "Con questo modello di regressione, la cui variabile dipendente è limitata asintoticamente tra 0 e 1, è possibile prevedere valori della label binari (\"0\" oppure \"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f6e42c3f-1240-499c-99fa-fbe8f7fedbeb",
    "_uuid": "a4739587-f3d5-4854-be1c-7e42df53df31"
   },
   "outputs": [],
   "source": [
    "parameters={'C':[10,7.5,6,5,4,3,1,0.5],'solver': ['lbfgs'],'max_iter':[200]}\n",
    "clf = LogisticRegression()\n",
    "# istanziazione di una griglia di classificatori sulla base del dizionario dei parametri (parameters)\n",
    "clf=GridSearchCV(clf,parameters,refit=True,cv=kf)\n",
    "clf.fit(features_train_norm,labels_train_norm)\n",
    "print(\"Miglior classificatore LogisticRegression \",clf.best_params_)\n",
    "best_par=clf.best_params_\n",
    "df_perf_LogReg=pd.DataFrame() \n",
    "mean_accuracies=clf.cv_results_['mean_test_score']\n",
    "parameters_tested=clf.cv_results_['params']\n",
    "type_clf=\"LogisticRegression\"\n",
    "clf_type_list=[]\n",
    "for i in range(len(parameters_tested)):\n",
    "    clf_type_list.append(type_clf)\n",
    "#Costruzione DataFrame delle performance di accuratezza in fase di validazione\n",
    "df_perf_LogReg['type'] = clf_type_list\n",
    "df_perf_LogReg['parameters']=parameters_tested\n",
    "df_perf_LogReg['performances']=mean_accuracies\n",
    "#Dichiarazione del miglior classificatore (per accuratezza in fase di validazione)\n",
    "best_clf=LogisticRegression(C=best_par['C'])\n",
    "best_clf.fit(features_train_norm,labels_train_norm)\n",
    "acc=accuracy_score(fitted_preds,labels_train_norm)\n",
    "#Previsioni dataset di test\n",
    "preds_LogReg=best_clf.predict(features_test_norm)\n",
    "#Costruzione di un DataFrame delle previsioni del miglior modello del tipo, e creazione di un file sottoponibile a Kaggle\n",
    "results=pd.DataFrame({'PassengerId':test_df['PassengerId'], 'Survived':preds_LogReg})\n",
    "results.to_csv(r\"C:\\Users\\Tommaso\\Desktop\\Titanic_prediction_LogReg.csv\", index=False)\n",
    "\n",
    "df_perf_LogReg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e5bb1f93-60ab-4ad3-953c-d697a5cf4f0f",
    "_uuid": "f067535a-0bb4-41e0-b8f2-d893535e65bb"
   },
   "source": [
    "Concateno i DataFrame dei classificatori utilizzati fino a qui, per confrontare le performances dei classificatori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "cc415812-648c-458e-8c67-e5de8165f493",
    "_uuid": "acea7a0a-8104-4f9e-bff6-c56cd24ae9fa"
   },
   "outputs": [],
   "source": [
    "df_perf_all=pd.concat([df_perf_NB,df_perf_SVM,df_perf_DT,df_perf_RF,df_perf_KNN,df_perf_LogReg])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b3b6a0dc-aa39-489c-9c99-36c244e852ec",
    "_uuid": "186a867a-1ce2-43bf-8959-c30944cbe486"
   },
   "source": [
    "La lista index_df_perf_all , costruita con il ciclo for contiene un range di numeri interi da 0 fino al numero totale di modelli in df_perf_all.\n",
    "Questa lista viene inserita come indice, in modo che ogni classificatore abbia un indice unico (attraverso l'istruzione df_perf_all.set_index(df_perf_all['new index']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "31be9bb4-a01d-45e6-9f1d-0611767c18f9",
    "_uuid": "cb5f6439-9c03-4221-a900-13d5416052f9"
   },
   "outputs": [],
   "source": [
    "index_df_perf_all=[]\n",
    "for it in range(df_perf_all.shape[0]):\n",
    "    index_df_perf_all.append(it)\n",
    "df_perf_all['new index']=index_df_perf_all\n",
    "df_perf_all=df_perf_all.set_index(df_perf_all['new index'])\n",
    "df_perf_all = df_perf_all.drop('new index', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "679dd482-737f-4d9e-95b8-5c06719d0631",
    "_uuid": "faf21f2a-672f-432c-b62c-c1bc636a2b31"
   },
   "outputs": [],
   "source": [
    "# Si selezionano i classificatori con un'accuratezza di convalida superiore a una certa soglia\n",
    "df_perf_eligible=df_perf_all.loc[(df_perf_all['performances'] > 0.834)]\n",
    "df_perf_eligible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a6fbc029-3e8a-4c11-a43b-f878306db605",
    "_uuid": "b68c3460-b88c-41dd-ab89-742aeaead7c9"
   },
   "outputs": [],
   "source": [
    "# Si selezionano i migliori classificatori di ogni tipo\n",
    "df_perf_group_best=df_perf_all.loc[df_perf_all.groupby('type').performances.idxmax()]\n",
    "df_perf_group_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "56d8447a-e072-4908-8f4d-16b3cf5d1fde",
    "_uuid": "40f9f3c3-f149-446a-8141-8a1088ad65ae"
   },
   "source": [
    "Si uniscono i due DataFrame precedenti, rimuovendo i duplicati. Così si ottengono i classificatori che ho ritenuto idonei a esprimere previsioni\n",
    "Come esempio di applicazione di un ensemble method, l'intenzione è quella di calcolare una media (ponderata) dei punteggi, tendendo conto di tutti i classificatori idonei.\n",
    "Così si potrebbe ottenere un meta-classificatore che potrebbe garantire una predizione più accurata e consistente.\n",
    "L'idea che qui si vuole implementare è simile al majority voting, in cui la predizione più frequente dei classificatori per un datapoint risulterà quella assunta dal meta-classificatore.\n",
    "Più in dettaglio, però, si è scelto di assegnare diversi pesi ai classificatori votanti, basati sulla loro accuratezza in fase di convalida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "75513bd7-0053-4589-bc3c-76c2619c079e",
    "_uuid": "baeeb6fd-d50c-45a8-aac0-4401e0875e6f"
   },
   "outputs": [],
   "source": [
    "# Si uniscono i dataframes e si rimuovono i duplicati\n",
    "df_perf_voting=pd.concat([df_perf_eligible,df_perf_group_best])\n",
    "df_perf_voting = df_perf_voting.reset_index()\n",
    "df_perf_voting=df_perf_voting.drop_duplicates(subset='new index')\n",
    "# Versione finale dei classificatori votanti\n",
    "df_perf_voting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "033ea889-c134-435f-a7c1-4f26a9f9cbf8",
    "_uuid": "ac294720-8dfd-41b3-ba12-67e28c8e5fcb"
   },
   "source": [
    "Assegnazione pesi ai classificatori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "40c9fcb0-5f2c-471e-9ee0-a0159ba8235f",
    "_uuid": "1c7f9bf8-4808-43dd-8d61-0559a88302b2"
   },
   "outputs": [],
   "source": [
    "# si calcola, come somma totale dei pesi, la somma delle statistiche di accuratezza in fase di convalida\n",
    "sum_weights=sum(df_perf_voting['performances'])\n",
    "# il peso \"elettorale\" di ogni classificatore è dato dal rapporto tra la sua accuratezza e la somma dei pesi. Così facendo,\n",
    "# si attribuisce una maggiore importanza ai classificatori più performanti \n",
    "df_perf_voting['weights']=df_perf_voting['performances']/sum_weights\n",
    "df_perf_voting=df_perf_voting.sort_values(['type', 'performances'], ascending=[True, False])\n",
    "index_df_perf_voting=[]\n",
    "# con le righe seguenti si riaggiorna in modo incrementalel'indice\n",
    "for it in range(df_perf_voting.shape[0]):\n",
    "    index_df_perf_voting.append(it)\n",
    "df_perf_voting['new index']=index_df_perf_voting\n",
    "df_perf_voting=df_perf_voting.set_index(df_perf_voting['new index'])\n",
    "df_perf_voting = df_perf_voting.drop('new index', 1)\n",
    "df_perf_voting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "662923fd-d800-4ff1-aebd-adc666c76e9f",
    "_uuid": "f3badbb2-37c0-402d-b7a1-b78b50ff5b10"
   },
   "source": [
    "Con la cella seguente si ottengono i vettori di previsione per ognuno dei classificatori votanti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "781806ac-69a9-4802-8728-0e4061d6b115",
    "_uuid": "f7bb0a0c-3279-465c-939b-3ef1a8ca66c6"
   },
   "outputs": [],
   "source": [
    "# Trasformo il DataFrame precedente in un dictionary per poter recuperare il tipo di classificatore, e i parametri di ogni singolo classificatore votante\n",
    "dict_clf_voting=df_perf_voting.to_dict('index')\n",
    "# predictions_vectors è la lista che verrà riempita dai vettori di previsione di ogni singolo classificatore (o modello di regressione)\n",
    "predictions_vectors=[]\n",
    "# per ogni elemento del dizionario l'obiettivo è ottenere il vettore di previsioni.\n",
    "# dato che ogni tipo di classificatore si basa su diversi parametri, bisogna prima specificare il tipo di classificatore utilizzato. \n",
    "# Per questo all'interno del ciclo for, è stata approntata una \"struttura\" ad \"if\"\n",
    "for item in dict_clf_voting:\n",
    "    if dict_clf_voting[item]['type']==\"GaussianNB\": #per esempio se il tipo di classificatore di un certo item è Gaussian Naive Bayes...\n",
    "        #... si istanzia un classificatore con il valore specifico di item del parametro 'prior' e poi di 'var_smoothing'\n",
    "        clf=GaussianNB(priors=dict_clf_voting[item]['parameters']['priors'],var_smoothing=dict_clf_voting[item]['parameters']['var_smoothing'])\n",
    "        clf.fit(features_train,labels_train) # si addestra sul train set questo specifico classificatore\n",
    "        preds_NB=clf.predict(features_test) # infine si calcolano le previsioni del dataset di test.\n",
    "        predictions_vectors.append(preds_NB) # Si aggiunge il vettore delle previsioni alla lista \"predictions_vectors\", che si popola scorrendo il ciclo for di dict_clf_voting\n",
    "        # si ripetono queste operazioni anche per gli altri tipi di classificatori, prestando attenzione alla necessità di normalizzare le features per gli algoritmi \"KNeighborsClassifier\"\n",
    "        # e \"svm.SVC\" (Support Vector Machine). Infatti, questi si basano sulle distanze tra i datapoints, e pertanto risentono (se non normalizzati o standardizzati) di feature che hanno differenti unità di misura e campo di variazione\n",
    "    if dict_clf_voting[item]['type']==\"KNeighborsClassifier\":\n",
    "        clf=KNeighborsClassifier(n_neighbors=dict_clf_voting[item]['parameters']['n_neighbors'],weights=dict_clf_voting[item]['parameters']['weights'])\n",
    "        clf.fit(features_train_norm,labels_train_norm)\n",
    "        preds_KNN=clf.predict(features_test_norm)\n",
    "        predictions_vectors.append(preds_KNN)\n",
    "    if dict_clf_voting[item]['type']==\"LogisticRegression\":\n",
    "        clf=LogisticRegression(C=dict_clf_voting[item]['parameters']['C'],solver=dict_clf_voting[item]['parameters']['solver'])\n",
    "        clf.fit(features_train_norm,labels_train_norm)\n",
    "        preds_KNN=clf.predict(features_test_norm)\n",
    "        predictions_vectors.append(preds_KNN)\n",
    "    if dict_clf_voting[item]['type']==\"RandomForestClassifier\":\n",
    "        clf=RandomForestClassifier(n_estimators=dict_clf_voting[item]['parameters']['n_estimators'],max_features=dict_clf_voting[item]['parameters']['max_features'],max_depth=dict_clf_voting[item]['parameters']['max_depth'],min_samples_split=dict_clf_voting[item]['parameters']['min_samples_split'],min_samples_leaf=dict_clf_voting[item]['parameters']['min_samples_leaf'])\n",
    "        clf.fit(features_train,labels_train)\n",
    "        preds_RF=clf.predict(features_test)\n",
    "        predictions_vectors.append(preds_RF)\n",
    "    if dict_clf_voting[item]['type']==\"svm.SVC\":\n",
    "        clf=svm.SVC(C=dict_clf_voting[item]['parameters']['C'],gamma=dict_clf_voting[item]['parameters']['gamma'],kernel=dict_clf_voting[item]['parameters']['kernel'])\n",
    "        clf.fit(features_train_norm,labels_train_norm)\n",
    "        preds_SVM=clf.predict(features_test_norm)\n",
    "        predictions_vectors.append(preds_SVM)\n",
    "    if dict_clf_voting[item]['type']==\"tree.DecisionTreeClassifier\":\n",
    "        clf=tree.DecisionTreeClassifier(max_depth=dict_clf_voting[item]['parameters']['max_depth'],min_samples_split=dict_clf_voting[item]['parameters']['min_samples_split'],min_samples_leaf=dict_clf_voting[item]['parameters']['min_samples_leaf'])\n",
    "        clf.fit(features_train,labels_train)\n",
    "        preds_DT=clf.predict(features_test)\n",
    "        predictions_vectors.append(preds_DT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "af55c01d-e462-477b-8aa8-26edc682a031",
    "_uuid": "9a3cf3fe-1157-4bd2-900d-4f9fa0394521"
   },
   "source": [
    "Si trasforma la lista dei singoli vettori (418 * 1) di previsione (della sopravvivenza delle persone nel dataset di test) in una matrice.\n",
    "Facendo il prodotto vettoriale tra i pesi e la matrice delle previsioni, si ottiene il vettore ensemble delle previsioni che è una media ponderata delle previsioni dei singoli classificatori. Il vettore risultante si chiama votes_float\n",
    "Dopodiché, è necessario arrotondare votes_float in un numero intero per ottenere i valori binari \"0\" e \"1\". Perché il vettore sia letto correttamente da kaggle infine è necessario convertirlo in maniera esplicita in interi tramite votes=np.round(votes_float,0).astype(int)\n",
    "votes è il vettore che esprime definitivamente le previsioni sul data set di test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4a8ffd77-55e3-4278-8b31-5890707f30ff",
    "_uuid": "a380f0dd-fb8b-4d4e-9bd4-c3c92a8514ee"
   },
   "outputs": [],
   "source": [
    "#trasformazione della lista dei vettori in una matrice\n",
    "prediction_matrix=np.array(predictions_vectors)\n",
    "#estrazione dei pesi e trasposizione per il successivo prodotto vettoriale\n",
    "weights=np.array(df_perf_voting['weights']).T\n",
    "#prodotto vettoriale tra i pesi e la matrice delle previsioni\n",
    "votes_float=np.dot(weights,prediction_matrix)\n",
    "votes_float_df=pd.DataFrame({'PassengerId':test_df['PassengerId'], 'Survived':votes_float})\n",
    "#arrotondamento del vettore ottenuto e conversione in tipi interi\n",
    "votes=np.round(votes_float,0).astype(int)\n",
    "results=pd.DataFrame({'PassengerId':test_df['PassengerId'], 'Survived':votes})\n",
    "results.to_csv(r\"C:\\Users\\Tommaso\\Desktop\\Titanic_prediction_Votes.csv\", index=False)\n",
    "print(\"Elections done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "12845ac4-e36c-433a-82ca-2590cb789ca5",
    "_uuid": "da41167d-4de3-4ec8-af34-44fd750ad686"
   },
   "outputs": [],
   "source": [
    "votes_float_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "cac83c73-e101-4761-9a32-14fa48957039",
    "_uuid": "98b7b71d-e4cc-404c-b229-fc94add9062d"
   },
   "outputs": [],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
